{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import credentials\n",
    "import requests\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pandas import DataFrame\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import calendar\n",
    "import datetime as dt\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "# pulling data API\n",
    "key =credentials.login['key_isaham']  # extract password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------PROCESSING GREATEC------------------\n",
      "----------------DONE GREATEC------------------\n",
      "----------------PROCESSING UWC------------------\n",
      "----------------DONE UWC------------------\n",
      "----------------PROCESSING D&O------------------\n",
      "----------------DONE D&O------------------\n",
      "----------------PROCESSING GENETEC------------------\n",
      "----------------DONE GENETEC------------------\n",
      "----------------PROCESSING UNISEM------------------\n",
      "----------------DONE UNISEM------------------\n",
      "----------------PROCESSING FRONTKN------------------\n",
      "----------------DONE FRONTKN------------------\n",
      "----------------PROCESSING ATAIMS------------------\n",
      "----------------DONE ATAIMS------------------\n",
      "----------------PROCESSING FPI------------------\n",
      "----------------DONE FPI------------------\n",
      "----------------PROCESSING PENTA------------------\n",
      "----------------DONE PENTA------------------\n",
      "----------------PROCESSING DUFU------------------\n",
      "----------------DONE DUFU------------------\n",
      "----------------PROCESSING VS------------------\n",
      "----------------DONE VS------------------\n",
      "----------------PROCESSING SKPRES------------------\n",
      "----------------DONE SKPRES------------------\n",
      "----------------PROCESSING MPI------------------\n",
      "----------------DONE MPI------------------\n",
      "----------------PROCESSING VITROX------------------\n",
      "----------------DONE VITROX------------------\n"
     ]
    }
   ],
   "source": [
    "# Define the instruments to download.\n",
    "tech = [\"GREATEC\",\n",
    "        \"UWC\",       \n",
    "        \"D&O\", \n",
    "       \"GENETEC\", \n",
    "       \"UNISEM\", \n",
    "       \"FRONTKN\", \n",
    "       \"ATAIMS\", \n",
    "       \"FPI\", \n",
    "       \"PENTA\", \n",
    "       \"DUFU\", \n",
    "       \"VS\", \n",
    "       \"SKPRES\", \n",
    "        \"MPI\", \n",
    "        \"VITROX\"\n",
    "       ]             \n",
    "# looping through all stocks\n",
    "\n",
    "# Scale dictionary of dataframes for training set(scaling by stock)\n",
    "ss={}\n",
    "d = {}\n",
    "\n",
    "# Scale dictionary of dataframes for prediction set(scaling by stock)\n",
    "sp={}\n",
    "nsp={}\n",
    "\n",
    "for m,stock in enumerate (tech):\n",
    "        \n",
    "    print(\"----------------PROCESSING {}------------------\".format(stock))\n",
    "    symbol = stock    # Which stock to pull    \n",
    "    response = requests.get(\"https://admin.isaham.my/api/chart?stock={}&key={}\".format(symbol,key))\n",
    "    data = response.json()\n",
    "    plt.style.use('fivethirtyeight')\n",
    "    %config InlineBackend.figure_format = 'retina'\n",
    "    %matplotlib inline\n",
    "\n",
    "    # Makes plots bigger\n",
    "    plt.rcParams[\"figure.figsize\"] = (20,8)\n",
    "    \n",
    "    # Tuning OB and OS level\n",
    "    ob_cross=85\n",
    "    os_cross=35\n",
    "\n",
    "    df= pd.DataFrame.from_dict(data)\n",
    "    df= df[['c','d','h','l','o','v']]\n",
    "    df['d'] = pd.to_datetime(df['d'], dayfirst= True )\n",
    "    \n",
    "    # Identify MA50\n",
    "    n=50\n",
    "    ma50=[] \n",
    "\n",
    "    for i in range(len(df[\"c\"])-(n-1)):\n",
    "        ma = round(sum(df[\"c\"][i:i+(n)])/n,4)\n",
    "        ma50.append(ma)\n",
    "    \n",
    "    df[\"MA50\"] = pd.Series(ma50)\n",
    "    df[\"MA50\"] = df[\"MA50\"].shift(n-1)\n",
    "\n",
    "    # Filter uptrend price above ma50\n",
    "    # Calculating the gap of both MA\n",
    "    df[\"MAgap\"] = df[\"c\"] -  df[\"MA50\"] \n",
    "    \n",
    "    # Set date as index\n",
    "    df.set_index('d',inplace=True,drop=False)\n",
    "    df.index.name = None\n",
    "\n",
    "    # Create signal for uptrend price above ma50 \n",
    "    # Result not accurate for stochastic when just filter uptrend only\n",
    "    df[\"Uptrend\"]=df['MAgap']>0\n",
    "    Uptrend=df[\"Uptrend\"]\n",
    "    Uptrend_signal=Uptrend[Uptrend==True]\n",
    "    \n",
    "    # Signal stochastic\n",
    "    # Create the \"L14\" column in the DataFrame\n",
    "    df['L14'] = df['l'].rolling(window=14).min()\n",
    "\n",
    "    # Create the \"H14\" column in the DataFrame\n",
    "    df['H14'] = df['h'].rolling(window=14).max()\n",
    "\n",
    "    # Create the \"%K\" column in the DataFrame\n",
    "    df['%K'] = 100*((df['c'] - df['L14']) / (df['H14'] - df['L14']) )\n",
    "\n",
    "    # Create the \"%D\" column in the DataFrame\n",
    "    df['%D'] = df['%K'].rolling(window=3).mean()\n",
    "\n",
    "    # Create a column in the DataFrame showing \"TRUE\" if buy entry signal is given and \"FALSE\" otherwise. \n",
    "    # A buy is initiated when the %K line crosses up through the %D line and the value of the oscillator is below 20 \n",
    "    df['Buy OS'] = ((df['%K'] > df['%D']) & (df['%K'].shift(1) < df['%D'].shift(1))) & (df['%D'] < os_cross) & (Uptrend_signal)\n",
    "\n",
    "    # Create a column in the DataFrame showing \"TRUE\" if sell entry signal is given and \"FALSE\" otherwise. \n",
    "    # A sell is initiated when the %K line crosses down through the %D line and the value of the oscillator is above 80 \n",
    "    df['Sell OB'] = ((df['%K'] < df['%D']) & (df['%K'].shift(1) > df['%D'].shift(1))) & (df['%D'] > ob_cross) \n",
    "    \n",
    "    # Create a column in the DataFrame for name of stock\n",
    "    df['Stock_Name'] = symbol\n",
    "\n",
    "    # Define overbought and oversold\n",
    "    overbought =df['Sell OB']\n",
    "    oversold =df['Buy OS']\n",
    "    \n",
    "    # Use index(date) for create strategy using overbought and oversold\n",
    "    date_os=list(oversold[oversold == True].index)\n",
    "    date_ob=list(overbought[overbought == True].index)\n",
    "    \n",
    "    # Scaling certain features only\n",
    "    col_names_pred = ['c', 'l', 'o', 'v','h']  \n",
    "    features_pred = df[col_names_pred]\n",
    "    \n",
    "    # Scaling for prediction data\n",
    "    scaler_pred = StandardScaler().fit(features_pred.values)\n",
    "    features_pred = scaler_pred.transform(features_pred.values)\n",
    "    \n",
    "    # Convert the array back to a dataframe\n",
    "    dataset_pred = DataFrame(features_pred)\n",
    "    \n",
    "    # Prepare prediction data that no need to scale\n",
    "    col_notscale_pred=['d','Stock_Name','Buy OS']\n",
    "    features_notscale_pred=df[col_notscale_pred]\n",
    "    \n",
    "    # Saving in multiple variable\n",
    "    sp[\"{}\".format(stock)] = dataset_pred\n",
    "    nsp[\"{}\".format(stock)] = features_notscale_pred\n",
    "    \n",
    "    # Transaction recording\n",
    "    buy = []\n",
    "    sell = []\n",
    "    sdate = []\n",
    "    bdate = []\n",
    "    wr = []\n",
    "    pct = []\n",
    "    vol = []\n",
    "    ope =[]\n",
    "    high = []\n",
    "    low = []\n",
    "\n",
    "    # Record close,open,vol,high,low for buy transaction\n",
    "    for n,i in enumerate(date_os):\n",
    "        buy.append(df['c'].loc[i])\n",
    "        vol.append(df['v'].loc[i])\n",
    "        ope.append(df['o'].loc[i])\n",
    "        high.append(df['h'].loc[i])\n",
    "        low.append(df['l'].loc[i])\n",
    "        bdate.append(i)\n",
    "        r=[]\n",
    "        \n",
    "        for m,k in enumerate(date_ob):\n",
    "            \n",
    "            if k>i:\n",
    "                r.append(m)\n",
    "                if len(r) ==1:\n",
    "                    # Record close,percentage change for sell transaction\n",
    "                    sell.append(df['c'].loc[k])\n",
    "                    sdate.append(k)\n",
    "                    pct_change=((df['c'].loc[k]/df['c'].loc[i])-1)*100\n",
    "                    pct.append(((df['c'].loc[k]/df['c'].loc[i])-1)*100)\n",
    "                elif len(r)>1:\n",
    "                    pass\n",
    "    diff =len(buy)-len(sell)\n",
    "    # Checking for final transaction, make sure it is sell, as we want to close the position\n",
    "    if len(buy) == len(sell):\n",
    "        pass   \n",
    "    elif len(buy) != len(sell):\n",
    "        # Deleting excess buy transaction after we close position\n",
    "        del(buy[-diff:])\n",
    "        del(bdate[-diff:])\n",
    "        del(ope[-diff:])\n",
    "        del(high[-diff:])\n",
    "        del(vol[-diff:])\n",
    "        del(low[-diff:])\n",
    "    dct = {\n",
    "        \"Buy_Date\": bdate,\n",
    "        \"Buy\": buy,\n",
    "        \"Sell\": sell,\n",
    "        \"Sell_Date\": sdate,\n",
    "        \"Open\":ope,\n",
    "        'Volume': vol,\n",
    "        \"Low\": low,\n",
    "        \"high\": high,\n",
    "        \"Pct_change\": pct    \n",
    "    }  \n",
    "    # Save document for stochastic strategy\n",
    "    trans = pd.DataFrame(dct)\n",
    "    \n",
    "    # Define variable for winning rate\n",
    "    wr = []\n",
    "    for i in range(len(trans)):\n",
    "        if trans[\"Pct_change\"][i] > 0:\n",
    "            wr.append(1)\n",
    "        else:\n",
    "            wr.append(0)\n",
    "    \n",
    "    # Define variable for name of stock\n",
    "    Name_stock = []\n",
    "    for s in range(len(trans)):\n",
    "        Name_stock.append(symbol)\n",
    "        \n",
    "    # Add new column\n",
    "    trans[\"wr\"] = pd.Series(wr)\n",
    "    trans['Stock'] = pd.Series(Name_stock)\n",
    "    \n",
    "    # Scaling certain features only for training data\n",
    "    col_names = ['Buy', 'Low', 'Open', 'Volume','high']  \n",
    "    features = trans[col_names]\n",
    "    \n",
    "    # Prepare prediction data that no need to scale\n",
    "    scaler = StandardScaler().fit(features.values)\n",
    "    features = scaler.transform(features.values)\n",
    "    \n",
    "    # Convert the array back to a dataframe\n",
    "    dataset = DataFrame(features)    \n",
    "    col_notscale = ['Buy_Date','Stock','wr']\n",
    "    features_notscale = trans[col_notscale]\n",
    "    \n",
    "    # Saving in multiple variable\n",
    "    d[\"{}\".format(stock)] = dataset\n",
    "    ss[\"{}\".format(stock)] = features_notscale\n",
    "    print(\"----------------DONE {}------------------\".format(symbol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining all training data generated\n",
    "df_notscale = pd.concat(list(ss.values()),ignore_index=True)\n",
    "df_scale = pd.concat(list(d.values()),ignore_index=True)\n",
    "df_scale=df_scale.rename(columns={0: 'Buy', 1: 'Low',2: 'Open', 3: 'Volume',4: 'High'})\n",
    "result_scaler = pd.concat([df_notscale, df_scale], axis=1)\n",
    "\n",
    "# Setting Dates as index\n",
    "df_all = result_scaler.sort_values(by=\"Buy_Date\")\n",
    "df_all.set_index('Buy_Date', inplace=True)\n",
    "\n",
    "# Encoding the stock names\n",
    "le = LabelEncoder()\n",
    "df_all[\"Stock\"] = le.fit_transform(df_all[\"Stock\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the splitting date (for training we use 2 years data)\n",
    "# Splitting train test\n",
    "day = dt.datetime.today().day\n",
    "month = dt.datetime.today().month\n",
    "year = dt.datetime.today().year - 2\n",
    "date1 = \"{}-{}-01\".format(year, str(month).zfill(2), str(day).zfill(2))\n",
    "date2 = str(dt.datetime.today().date() - dt.timedelta(days=1))  # until yesterday only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate input features and target\n",
    "X = df_all.drop(['wr'], axis=1)\n",
    "y = df_all.wr\n",
    "\n",
    "# Splitting train test\n",
    "X_train, y_train = X[date1: date2], y[date1: date2]\n",
    "\n",
    "# Resampling\n",
    "# Concatenate training data back together\n",
    "X_concat = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# Separate minority and majority classes\n",
    "not_win = X_concat[X_concat.wr==0]\n",
    "win = X_concat[X_concat.wr==1]\n",
    "\n",
    "# Upsample minority\n",
    "not_win_upsampled = resample(not_win,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=len(win), # match number in majority class\n",
    "                          random_state=0) # reproducible results\n",
    "\n",
    "# Combine majority and upsampled minority\n",
    "upsampled = pd.concat([win, not_win_upsampled]).sort_values(by=\"Buy_Date\")\n",
    "\n",
    "# Using logistic regression again with the balanced dataset\n",
    "X_train = upsampled.drop('wr', axis=1)\n",
    "y_train = upsampled.wr\n",
    "\n",
    "# Training the model\n",
    "upsampled = LogisticRegression(solver='liblinear').fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining all prediction data generated\n",
    "df_notscale_pred = pd.concat(list(nsp.values()),ignore_index=True)\n",
    "df_scale_pred = pd.concat(list(sp.values()),ignore_index=True)\n",
    "\n",
    "# Rename column\n",
    "df_scale_pred = df_scale_pred.rename(columns={0: 'Buy', 1: 'Low',2: 'Open', 3: 'Volume',4: 'High'})\n",
    "\n",
    "# Concat all\n",
    "result_scaler_pred = pd.concat([df_notscale_pred, df_scale_pred], axis=1)\n",
    "result_scaler_pred = result_scaler_pred.rename(columns={'d': 'Date'})\n",
    "\n",
    "# Setting Dates as index\n",
    "df_all_pred = result_scaler_pred.sort_values(by=\"Date\")\n",
    "df_all_pred.set_index('Date', inplace=True)\n",
    "\n",
    "# Encoding the stock names\n",
    "le = LabelEncoder()\n",
    "df_all_pred[\"Stock\"] = le.fit_transform(df_all_pred[\"Stock_Name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKPRES shows No buy signal.\n",
      " No prediction\n",
      "DUFU shows No buy signal.\n",
      " No prediction\n",
      "VITROX shows No buy signal.\n",
      " No prediction\n",
      "GREATEC shows No buy signal.\n",
      " No prediction\n",
      "ATAIMS shows No buy signal.\n",
      " No prediction\n",
      "PENTA shows No buy signal.\n",
      " No prediction\n",
      "FRONTKN shows No buy signal.\n",
      " No prediction\n",
      "MPI shows No buy signal.\n",
      " No prediction\n",
      "FPI shows No buy signal.\n",
      " No prediction\n",
      "GENETEC shows No buy signal.\n",
      " No prediction\n",
      "VS shows No buy signal.\n",
      " No prediction\n",
      "D&O shows No buy signal.\n",
      " No prediction\n",
      "UWC shows No buy signal.\n",
      " No prediction\n",
      "UNISEM shows No buy signal.\n",
      " No prediction\n",
      "DUFU shows No buy signal.\n",
      " No prediction\n",
      "PENTA shows No buy signal.\n",
      " No prediction\n",
      "VS shows No buy signal.\n",
      " No prediction\n",
      "FPI shows No buy signal.\n",
      " No prediction\n",
      "GREATEC shows No buy signal.\n",
      " No prediction\n",
      "ATAIMS shows No buy signal.\n",
      " No prediction\n",
      "SKPRES shows No buy signal.\n",
      " No prediction\n",
      "FRONTKN shows No buy signal.\n",
      " No prediction\n",
      "UNISEM shows No buy signal.\n",
      " No prediction\n",
      "GENETEC shows No buy signal.\n",
      " No prediction\n",
      "D&O shows No buy signal.\n",
      " No prediction\n",
      "UWC shows No buy signal.\n",
      " No prediction\n",
      "MPI shows No buy signal.\n",
      " No prediction\n",
      "VITROX shows No buy signal.\n",
      " No prediction\n"
     ]
    }
   ],
   "source": [
    "# Taking today and yesterday data only\n",
    "latest = df_all_pred[date2:]\n",
    "\n",
    "# Record prediction result\n",
    "post = {}\n",
    "\n",
    "for m in range(len(latest)):\n",
    "    if latest[\"Buy OS\"].iloc[m] == True:\n",
    "        \n",
    "        # Assigning the dictionary for each stocks\n",
    "        dct_1 = {}\n",
    "\n",
    "        # predicting the test set\n",
    "        inputs = latest.drop(['Stock_Name','Buy OS'] ,axis=1).iloc[m].values\n",
    "        inputs = inputs.reshape(1,-1)\n",
    "        \n",
    "        # Test the model\n",
    "        y_pred =upsampled.predict(inputs)\n",
    "        pred1 = [round(value) for value in y_pred]\n",
    "        print(pred1)\n",
    "        \n",
    "        # Saving the dct format\n",
    "        dct_1[\"Signal\"] = \"1\"\n",
    "        dct_1[\"Predicted_Class\"] =  pred1[0]\n",
    "        dct_1[\"Date_predicted\"] = dt.datetime.today()\n",
    "        print(\"{} shows buy signal... Producing the prediction\".format(latest[\"Stock_Name\"].iloc[m]))\n",
    "        # Append dict in post\n",
    "        post[\"{}\".format(latest[\"Stock_Name\"].iloc[m])] = dct_1   \n",
    "    else:\n",
    "        dct_1[\"Signal\"] = \"0\"\n",
    "        dct_1[\"Predicted_Class\"] = '0'\n",
    "        dct_1[\"Date_predicted\"] = dt.datetime.today()\n",
    "        \n",
    "        \n",
    "        print(\"{} shows No buy signal.\\n No prediction\".format(latest[\"Stock_Name\"].iloc[m]))\n",
    "        # Append dict in post\n",
    "        post[\"{}\".format(latest[\"Stock_Name\"].iloc[m])] = dct_1\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SKPRES': {'Signal': '0',\n",
       "  'Predicted_Class': '0',\n",
       "  'Date_predicted': datetime.datetime(2021, 4, 8, 17, 12, 44, 701923)},\n",
       " 'DUFU': {'Signal': '0',\n",
       "  'Predicted_Class': '0',\n",
       "  'Date_predicted': datetime.datetime(2021, 4, 8, 17, 12, 44, 701923)},\n",
       " 'VITROX': {'Signal': '0',\n",
       "  'Predicted_Class': '0',\n",
       "  'Date_predicted': datetime.datetime(2021, 4, 8, 17, 12, 44, 701923)},\n",
       " 'GREATEC': {'Signal': '0',\n",
       "  'Predicted_Class': '0',\n",
       "  'Date_predicted': datetime.datetime(2021, 4, 8, 17, 12, 44, 701923)},\n",
       " 'ATAIMS': {'Signal': '0',\n",
       "  'Predicted_Class': '0',\n",
       "  'Date_predicted': datetime.datetime(2021, 4, 8, 17, 12, 44, 701923)},\n",
       " 'PENTA': {'Signal': '0',\n",
       "  'Predicted_Class': '0',\n",
       "  'Date_predicted': datetime.datetime(2021, 4, 8, 17, 12, 44, 701923)},\n",
       " 'FRONTKN': {'Signal': '0',\n",
       "  'Predicted_Class': '0',\n",
       "  'Date_predicted': datetime.datetime(2021, 4, 8, 17, 12, 44, 701923)},\n",
       " 'MPI': {'Signal': '0',\n",
       "  'Predicted_Class': '0',\n",
       "  'Date_predicted': datetime.datetime(2021, 4, 8, 17, 12, 44, 701923)},\n",
       " 'FPI': {'Signal': '0',\n",
       "  'Predicted_Class': '0',\n",
       "  'Date_predicted': datetime.datetime(2021, 4, 8, 17, 12, 44, 701923)},\n",
       " 'GENETEC': {'Signal': '0',\n",
       "  'Predicted_Class': '0',\n",
       "  'Date_predicted': datetime.datetime(2021, 4, 8, 17, 12, 44, 701923)},\n",
       " 'VS': {'Signal': '0',\n",
       "  'Predicted_Class': '0',\n",
       "  'Date_predicted': datetime.datetime(2021, 4, 8, 17, 12, 44, 701923)},\n",
       " 'D&O': {'Signal': '0',\n",
       "  'Predicted_Class': '0',\n",
       "  'Date_predicted': datetime.datetime(2021, 4, 8, 17, 12, 44, 701923)},\n",
       " 'UWC': {'Signal': '0',\n",
       "  'Predicted_Class': '0',\n",
       "  'Date_predicted': datetime.datetime(2021, 4, 8, 17, 12, 44, 701923)},\n",
       " 'UNISEM': {'Signal': '0',\n",
       "  'Predicted_Class': '0',\n",
       "  'Date_predicted': datetime.datetime(2021, 4, 8, 17, 12, 44, 701923)}}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining variables from environment variables\n",
    "user = credentials.login['username_db']\n",
    "pwd = credentials.login['password_db']\n",
    "ip = credentials.login['ip_db']\n",
    "port = credentials.login['port_db']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list of db ['MA5_efficiency', 'News_Keywords', 'News_Project', 'News_Project_01', 'News_Update', 'Stochastic_strategy', 'The_Star_Logs', 'The_Star_Webscrape', 'admin', 'config', 'fbp_predictions2', 'local', 'logs', 'syamil_test', 'test', 'test_db', 'xgb_pred']\n"
     ]
    }
   ],
   "source": [
    "#1. connect to cluster -> master_client = ......\n",
    "# Access to cluster\n",
    "master_client = MongoClient(\"mongodb://{}:{}@{}:{}\".format(user,pwd,ip,port))\n",
    "print(\"list of db\", master_client.list_database_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing mongodb database\n",
    "db = master_client['Stochastic_strategy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Collections before:  1\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Reviewing existed collections\n",
    "print(\"No. of Collections before: \", len(db.list_collection_names()))\n",
    "print(\"----------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new collections for each predicted values\n",
    "col = db[str(dt.datetime.today().date())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertOneResult at 0x2280ec24500>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Insert forecasted value in each collections\n",
    "col.insert_one(post)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document after  2\n"
     ]
    }
   ],
   "source": [
    "#counting document after\n",
    "post_count = col.count_documents({})\n",
    "print('document after ',post_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
